\documentclass[11pt]{article}
\usepackage[final]{acl}
% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[]{acl}
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Group 50 Progress Report:\\Neural Network-Based Wildfire Risk Classification}


\author{Eric Solak, Tyler Yue, Ahmed Sahi \\
  \texttt{\{solake, yuet5, sahia8\}@mcmaster.ca} }

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
% \begin{abstract}
% \end{abstract}

\section{Introduction}

Wildfires are a growing threat to infrastructure, ecosystems, and human safety. In addition to the immediate danger of fire, wildfire smoke can travel thousands of kilometers, degrading air quality and affecting
millions of people \citet{NASAwildfires}. Predicting the likelihood of wildfires in advance can help authorities, first responders, and communities with preparation, resource allocation, and reduce potential damage.

This project aims to classify regions into low, medium, and high wildfire risk using a machine learning model trained on environmental and meteorological data. The resulting risk predictions can support proactive
decision making and reduce the overall impact of wildfires on both people and the environment.

\section{Related Work}

Predicting wildfires is a sought after problem in both environmental science and machine learning. Existing approaches can be categorized into deterministic models, statistical models, and machine learning models.

Deterministic models, such as the Canadian Forest Fire Danger Rating System (CFFDRS) rely on empirical formulae derived from field experiments to estimate wildfire risk based on meteorological factors \citet{cffdrs}. These models are
generally used in operational settings but are limited in their ability to capture non-linear interactions between variables.

Statistical models include regression-based and Bayesian approaches that predict wildfire likelihood using historical fire and weather data \citet{statisticalregression2024, bayesian2019}. These methods are effective in identifying trends but they are limited in
their ability to capture rare or extreme weather conditions, which is critical for high-risk classification.

Machine learning approaches have used tree-ensemble methods such as XGBoost to classify wildfire risk based on tabular environmental data \citet{XGBoost}. More recently however, neural networks have been trained on both satellite imagery
and meteorological data to predict wildfire spread \citet{MLWildfire}. These models can learn complex, non-linear relationships but require preprocessing and normalization of inputs.

\section{Dataset}
For this project, we will utilize a Wildfire Prediction Dataset found on Kaggle \citet{kaggle_dataset}. This dataset contains 118858 entries and 17 attributes, with no missing values. Further, all values are numerical (float64), 
simplifying any data manipulation by avoiding issues revolving around categorical data. 
\subsection{Data Sources}
There are two main sources used by this dataset: NASA's FIRMS VIIRS SCC system for fire radiative power (FRP) data, 
and Open-Meteo's meteorological data \citet{FIRMS, open-meteo-api}. As these sources are open and publicly available, the data used is compliant with all terms of service.
\subsection{Features}\label{Features}
The dataset contains the following features:
\begin{itemize}
  \item \textbf{daynight\_N}: Indicator for whether the observation was taken during the day or night.
  \item \textbf{lat, lon}: Latitude and longitude coordinates of the location where the observation was recorded.
  \item \textbf{fire\_weather\_index}: An index representing the overall fire risk based on meteorological conditions.
  \item \textbf{pressure\_mean}: Mean atmospheric pressure at the location.
  \item \textbf{wind\_direction\_mean, wind\_direction\_std}: Mean and standard deviation of wind direction.
  \item \textbf{solar\_radiation\_mean}: Average solar radiation received.
  \item \textbf{dewpoint\_mean}: Average dew point temperature.
  \item \textbf{cloud\_cover\_mean}: Average cloud cover percentage.
  \item \textbf{evapotranspiration\_total}: Total evapotranspiration over the observation period.
  \item \textbf{humidity\_min}: Minimum humidity recorded.
  \item \textbf{temp\_mean, temp\_range}: Mean temperature and temperature range.
  \item \textbf{wind\_speed\_max}: Maximum wind speed observed.
  \item \textbf{occured}: Binary flag indicating whether a fire occurred (1) or not (0).
  \item \textbf{frp}: Fire Radiative Power, measuring the intensity of a fire if it occurred.
\end{itemize}

\subsection{Preprocessing}
The dataset requires minimal preprocessing due to its numerical format. The preprocessing steps used are:
\begin{itemize}
  \item Clipping negative values: Negative \textit{fire\_weather\_index} values are set to 0, as negative FWI is meaningless (i.e., negative risk is nonsensical).
  \item Feature normalization: All numeric features are standardized to facilitate stable training of the neural network. Each feature was scaled to have a mean of 0
  and a standard deviation of 1, ensuring that features with larger ranges do not dominate the training process.
\end{itemize}

\subsection{Data Split}
A standard training/validation/testing split will be used, 70\% for training, 15\% for validation, 15\% for testing.

\section{Features}

All 17 numerical features are used as input to the neural network. These features, described in Section \ref{Features}, include positional, meteorological and environmental data.
Positional features (\textit{lat}, \textit{lon}) provide the geographical context, important to the input as wildfire risk is often region-specific due to climate and vegetation.
Meteorological features, such as \textit{temp\_mean}, \textit{temp\_range}, \textit{humidity\_min}, \textit{pressure\_mean}, \textit{cloud\_cover\_mean}, \textit{dewpoint\_mean}, \textit{solar\_radiation\_mean}, etc,
describe the atmospheric conditions that directly influence fire ignition and propagation. Environmental features, including \textit{evapotranspiration\_total}, \textit{fire\_weather\_index}, etc, describe the conditions
that influence fire behaviour and ignition potential. 

The model's target variable is the wildfire risk category, derived from the \textit{fire\_weather\_index} and \textit{occurred} variables in the dataset,
classified into low, medium, or high risk. By combining multiple features that capture positional, meteorological, and environmental conditions, the neural network can learn complex, non-linear relationships 
that contribute to wildfire risk. 

\section{Implementation}

This classifier was implemented in Python using the PyTorch and Scikit-learn libraries. The system was broken up into three modules: preprocessing, model training, and evaluation, each implemented in a separate Jupyter notebook.

\subsection{Preprocessing}

In the \texttt{01\_preprocessing.ipynb} notebook, the dataset is loaded and cleaned to remove any empty rows. Since Fire Weather Index (FWI) is meant to be non-negative, all negative values were set to 0. In next iterations, extreme values on the high end will also be corrected (as an FWI of 200, for example, is not physically possible). The data set was labeled high, medium, or low risk based on whether or not a fire occurred, and \textit{Natural Resources Canada}' FWI categorization. If a fire occurred or the FWI was greater than 30, the risk level was high. If no fire occurred, but the FWI was between 10 and 30, the risk level was medium. If no fire occurred and the FWI was below 10, the risk level was low. Each feature was standardized using the z-score transformation, ensuring that features of larger magnitude don't dominate gradient updates. A 70/15/15 split was applied for training, validation, and test datasets.

\subsection{Model Architecture}

The neural network, implemented in \texttt{02\_model\_training.ipynb}, uses a feed-forward MLP optimized for meteorological data. The model takes 17 input features and predicts the probability of belonging to one of the three wildfire risk classes. A combination of linear, ReLU, softmax and dropout functions will be used in this model. With further trials in next iterations, a final setup will be determined. The model was compiled using a learning rate of 0.001 and the categorical cross-entropy loss function. Class weighting was applied so that errors in underrepresented categories (e.g., medium-risk) incurred a higher penalty. Model training ran for up to 120 epochs with early stopping (patience = 10).

\section{Results and Evaluation}

Our model performs a three-way data split into training, test, and validation datasets. We used a stratified split to maintain the distribution of our target variable \textit{risk\_level} between the sets. The purpose of the splits are as follows:

\begin{enumerate}
    \item \textbf{Training set:} used to fit the model parameters and to update the weights during backpropagation.
    \item \textbf{Validation set:} used to determine performance during training and used for our early stop gradient descent and dynamic learning rate scheduling.
    \item \textbf{Test set:} used to evaluate the model performance on unseen data and provide an accurate measurement of our model's ability to generalize and predict on new data.
\end{enumerate}

Our evaluation metrics are based on the classification performance. We computed the precision, recall, and F1-score for each risk class, together with the overall accuracy and a confusion matrix. Accuracy serves as the primary evaluation metric, while the F1-score provides a more balanced measure of performance across all risk categories.

The neural-network classifier (three hidden layers with ReLU activations and Dropout) was trained on GPU for approximately 120 epochs with early stopping. The model achieved a validation accuracy of about 61 before convergence and a test accuracy of 63.7. The F1 scores show relatively good performance:

\begin{itemize}
    \item \textbf{Low-risk (class 0):} F1 = 0.68
    \item \textbf{Medium-risk (class 1):} F1 = 0.49
    \item \textbf{High-risk (class 2):} F1 = 0.66
\end{itemize}

The confusion matrix shows that most misclassifications are between medium and high-risk wildfires. Precision and recall vary for the each class in the range of 0.45 to 0.88. 

\section{Feedback and Plans}

During the project, our TA provided guidance on narrowing the scope of our wildfire risk classification task and ensuring that our model design aligns with the project requirements. 
In particular, we were advised to focus on using the numerical environmental and meteorological data effectively, rather than attempting to include complex satellite imagery or external data sources, 
which could overcomplicate the project given the time constraints. This advice helped us narrow the project scope and prioritize the features most relevant to predicting wildfire risk.
The TA also emphasized the importance of properly preprocessing features, specifically the negative and extreme values in \textit{fire\_weather\_index} to ensure stable neural network training.

Based on this feedback and reflections on the project's progress so far, our plan for the remaining weeks is to finalize the preprocessing pipeline to ensure all unrealistic values are handled appropriately (extreme \textit{fire\_weather\_index} values). 
We also plan on refining the presentation of the results and conduct exploratory analysis. This includes generating detailed visualizations of feature importance using partial dependence plots to explain the model's predictions 
for specific regions. Including these graphs in the final report will help communicate both the strengths and limitations of the model.

%\section{Template Notes}
%
%You can remove this section or comment it out, as it only contains instructions for how to use this template. You may use subsections in your document as you find appropriate.
%
%\subsection{Tables and figures}
%
%See Table~\ref{citation-guide} for an example of a table and its caption.
%See Figure~\ref{fig:experiments} for an example of a figure and its caption.
%
%
%\begin{figure}[t]
%  \includegraphics[width=\columnwidth]{example-image-golden}
%  \caption{A figure with a caption that runs for more than one line.
%    Example image is usually available through the \texttt{mwe} package
%    without even mentioning it in the preamble.}
%  \label{fig:experiments}
%\end{figure}
%
%\begin{figure*}[t]
%  \includegraphics[width=0.48\linewidth]{example-image-a} \hfill
%  \includegraphics[width=0.48\linewidth]{example-image-b}
%  \caption {A minimal working example to demonstrate how to place
%    two images side-by-side.}
%\end{figure*}
%
%
%\subsection{Citations}
%
%\begin{table*}
%  \centering
%  \begin{tabular}{lll}
%    \hline
%    \textbf{Output}           & \textbf{natbib command} & \textbf{ACL only command} \\
%    \hline
%    \citep{Gusfield:97}       & \verb|\citep|           &                           \\
%    \citealp{Gusfield:97}     & \verb|\citealp|         &                           \\
%    \citet{Gusfield:97}       & \verb|\citet|           &                           \\
%    \citeyearpar{Gusfield:97} & \verb|\citeyearpar|     &                           \\
%    \citeposs{Gusfield:97}    &                         & \verb|\citeposs|          \\
%    \hline
%  \end{tabular}
%  \caption{\label{citation-guide}
%    Citation commands supported by the style file.
%  }
%\end{table*}
%
%Table~\ref{citation-guide} shows the syntax supported by the style files.
%We encourage you to use the natbib styles.
%You can use the command \verb|\citet| (cite in text) to get ``author (year)'' citations, like this citation to a paper by \citet{Gusfield:97}.
%You can use the command \verb|\citep| (cite in parentheses) to get ``(author, year)'' citations \citep{Gusfield:97}.
%You can use the command \verb|\citealp| (alternative cite without parentheses) to get ``author, year'' citations, which is useful for using citations within parentheses (e.g. \citealp{Gusfield:97}).

% \section*{Limitations}

\section*{Team Contributions}
\begin{itemize}
  \item \textbf{Eric Solak: } Created Team Contract. Authored and completed Project Proposal sections \textit{Overview}, and \textit{Task Definition}. Set up the GitHub repository, including modularization, project structuring, separation of concerns. 
  Authored and completed Project Progress Report sections 1 \textit{Introduction}, 2 \textit{Related Work}, 3 \textit{Dataset}, 4 \textit{Features}, and 7 \textit{Feedback and Plans}.
  \item \textbf{Tyler Yue: } Authored and completed Project Proposal section \textit{Results and Evaluation}. Coded parts of the model including the selection of layers in the neural network and adding a class weighting to improve accuracy by modifying the loss penalty for classess with less datapoints.
  \item \textbf{Ahmed Sahi: } Coded the preprocessing steps in notebook 1, the model setup in notebook 2, and the evaluation metrics in notebook 3. Authored and completed Project Proposal section \textit{Implementation}.
\end{itemize}
% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{custom,anthology-overleaf-1,anthology-overleaf-2}

% Custom bibliography entries only
\bibliography{custom}

% \appendix

% \section{Example Appendix}
% \label{sec:appendix}

% This is an appendix.

\end{document}
